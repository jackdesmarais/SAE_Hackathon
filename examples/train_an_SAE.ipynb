{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################### 1. General setup #######################\n",
    "################### Mostly not model specific ##############\n",
    "############################################################\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import argparse\n",
    "import torch\n",
    "from SAE_models import get_cfg, TopKSAE, VanillaSAE, JumpReLUSAE, BatchTopKSAE\n",
    "from SAE_training import SAETraining\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# SpliceAI specific imports\n",
    "from datasets import LiaoDatasetEmbedded\n",
    "from SpliceAI import SpliceAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'architecture_search_env (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Add spliceAI specific arguments\n",
    "\n",
    "spliceai_args = {\n",
    "    'k': 32,  # Number of filters in convolutional layers\n",
    "    'wsets': [11],  # Kernel widths for each MegaBlock\n",
    "    'dsets': [1],  # Dilation rates for each MegaBlock\n",
    "    'nt_dims': 4,  # Number of nucleotide dimensions\n",
    "    'output_dim': 3,  # Number of output dimensions\n",
    "    'dropout_rate': None,  # Dropout rate\n",
    "    'block_count': 4,  # Number of MegaBlocks\n",
    "    'hook_point': 'mb 1',  # Embedding layer to use\n",
    "    'embedding_dim': 32,  # Embedding dimension\n",
    "    'input_length': 176,  # Input length\n",
    "    'positions_to_use': [0,75],  # Positions to use for training\n",
    "    'csv_path': './data/Liao_Dataset/liao_training_set.csv',  # Path to training set CSV file\n",
    "    'plasmid_path': './data/Liao_Dataset/liao_plasmid.gbk',  # Path to plasmid file\n",
    "    'test_csv_path': './data/Liao_Dataset/liao_test_set.csv',  # Path to test set CSV file\n",
    "    'flanking_len': 6,  # Flanking length\n",
    "    'add_context_len': True,  # Are positions relative to input length rather than context length\n",
    "    'auto_find_bc_pos': True,  # Enable automatic finding of BC positions\n",
    "    'auto_find_ex_pos': True,  # Enable automatic finding of exon positions\n",
    "    'preload': True,  # Enable data preloading\n",
    "    'preload_embeddings': True,  # Enable embeddings preloading\n",
    "    'num_workers': 16,  # Number of workers for data loading\n",
    "    'spliceai_batch_size': 100,  # Batch size for training\n",
    "    'h5_file': 'SpliceAI_Models/SpliceNet80_g1.h5',  # Path to SpliceAI model file\n",
    "    'model_name': 'SpliceAI'  # Name of the model\n",
    "}\n",
    "\n",
    "cfg = get_cfg(spliceai_args)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'architecture_search_env (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = SAETraining(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'architecture_search_env (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Setup the SpliceAI embedding dataset\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    Convert DNA sequence to one-hot encoded tensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Input sequence array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        One-hot encoded tensor of shape (4, sequence_length)\n",
    "    \"\"\"\n",
    "    var_ar = x[:,None] == np.array(['A','C','G', 'T'])\n",
    "    var_ar = var_ar.T\n",
    "    var_t = torch.Tensor(var_ar).float()\n",
    "    return(var_t)\n",
    "\n",
    "\n",
    "spliceai_model = SpliceAI(k=cfg['k'], \n",
    "                    wsets=cfg['wsets'], \n",
    "                    dsets=cfg['dsets'], \n",
    "                    nt_dims=cfg['nt_dims'], \n",
    "                    output_dim=cfg['output_dim'], \n",
    "                    dropout_rate=cfg['dropout_rate'], \n",
    "                    block_count=cfg['block_count'], \n",
    "                    embedding_layer=cfg['hook_point'], \n",
    "                    embedding_dim=cfg['embedding_dim'], \n",
    "                    input_length=cfg['input_length'], \n",
    "                    positions_to_use=cfg['positions_to_use'])\n",
    "\n",
    "spliceai_model.load_from_h5_file(cfg['h5_file'])\n",
    "spliceai_model.mode = 'embed'\n",
    "\n",
    "cfg['context_len'] = spliceai_model.cl\n",
    "\n",
    "if cfg['add_context_len']:\n",
    "    cfg['positions_to_use'] = [pos+cfg['context_len']//2 for pos in cfg['positions_to_use']]\n",
    "\n",
    "\n",
    "transform_x = one_hot_encode\n",
    "\n",
    "full_train_ds = LiaoDatasetEmbedded(cfg['csv_path'], \n",
    "                            cfg['plasmid_path'], \n",
    "                            cfg['context_len']+cfg['flanking_len'], \n",
    "                            spliceai_model, \n",
    "                            cfg['auto_find_bc_pos'],\n",
    "                            auto_find_ex_pos=cfg['auto_find_ex_pos'], \n",
    "                            batch_size=cfg['batch_size'], \n",
    "                            transform_x=transform_x, \n",
    "                            preload=cfg['preload'], \n",
    "                            preload_embeddings=cfg['preload_embeddings'], \n",
    "                            trainer=trainer.trainer, \n",
    "                            num_workers=cfg['num_workers'])\n",
    "full_train_ds.open()\n",
    "train_size = int(len(full_train_ds)*0.8)\n",
    "ids = np.random.permutation(len(full_train_ds))\n",
    "train_ds = torch.utils.data.dataset.Subset(full_train_ds, ids[:train_size])\n",
    "train_dl = torch.utils.data.dataloader.DataLoader(train_ds, batch_size=cfg['spliceai_batch_size'], num_workers=cfg['num_workers'])\n",
    "val_ds = torch.utils.data.dataset.Subset(full_train_ds, ids[train_size:])\n",
    "val_dl = torch.utils.data.dataloader.DataLoader(val_ds, batch_size=cfg['spliceai_batch_size'], num_workers=cfg['num_workers'])\n",
    "\n",
    "test_ds = LiaoDatasetEmbedded(cfg['test_csv_path'], \n",
    "                            cfg['plasmid_path'], \n",
    "                            cfg['context_len']+cfg['flanking_len'], \n",
    "                            spliceai_model, \n",
    "                            cfg['auto_find_bc_pos'],\n",
    "                            auto_find_ex_pos=cfg['auto_find_ex_pos'], \n",
    "                            batch_size=cfg['batch_size'], \n",
    "                            transform_x=transform_x, \n",
    "                            preload=cfg['preload'], \n",
    "                            preload_embeddings=cfg['preload_embeddings'], \n",
    "                            trainer=trainer.trainer, \n",
    "                            num_workers=cfg['num_workers'])\n",
    "test_ds.open()\n",
    "test_dl = torch.utils.data.dataloader.DataLoader(test_ds, batch_size=cfg['spliceai_batch_size'], num_workers=cfg['num_workers'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################### 5. Training ############################\n",
    "################### Not Model specific #####################\n",
    "############################################################\n",
    "\n",
    "final_model = trainer.train(model, train_dl, val_dl)\n",
    "\n",
    "\n",
    "############################################################\n",
    "################### 6. testing/validation ##################\n",
    "################### Not Model specific #####################\n",
    "############################################################\n",
    "\n",
    "val_metrics = trainer.validate(val_dl)\n",
    "\n",
    "print(val_metrics)\n",
    "with open(cfg['outpath'] + f\"{cfg['name']}_{cfg['seed']}_val_metrics.json\", 'w') as f:\n",
    "    json.dump(val_metrics, f)\n",
    "\n",
    "test_metrics = trainer.test(test_dl)\n",
    "\n",
    "print(test_metrics)\n",
    "with open(cfg['outpath'] + f\"{cfg['name']}_{cfg['seed']}_test_metrics.json\", 'w') as f:\n",
    "    json.dump(test_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "architecture_search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
